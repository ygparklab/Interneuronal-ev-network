import argparse
from collections import OrderedDict
import itertools
import matplotlib
import pickle
matplotlib.use('Qt5Agg')
import numpy as np
import h5py
import json
import os
import sys
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from PyQt5 import QtCore, QtWidgets, QtGui
from PyQt5.QtWidgets import QShortcut
from PyQt5.QtGui import QKeySequence
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
from matplotlib.figure import Figure
import tqdm
from nuggt.utils.ngutils import red_shader, green_shader, blue_shader, layer
from nuggt.utils.ngutils import gray_shader, jet_shader, cubehelix_shader

COLORS = OrderedDict(
              red=red_shader,
              green=green_shader,
              blue=blue_shader,
              gray=gray_shader,
              jet=jet_shader,
              cubehelix=cubehelix_shader)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--patch-file",
                        required=True,
                        action="append",
                        help="The patch file generated by collect-patches, here check")
    parser.add_argument("--output",
                        required=True,
                        help="The random forest model's pickle file")
    parser.add_argument("--n-components",
                        type=int,
                        default=32,
                        help="The number of components for the PCA "
                        "dimensionality reduction.")
    parser.add_argument("--use-position",
                        help="Add the X, Y and Z location of a putative cell "
                        "to the feature vector to allow the classifier to use "
                        "position in the decision process.",
                        action="store_true")
    parser.add_argument("--whiten",
                        action="store_true",
                        help="Normalize data in the PCA decomposition.")
    parser.add_argument("--max-samples",
                        type=int,
                        default=1000000,
                        help="The maximum number of samples for PCA "
                        "(subsample if there are more).")
    parser.add_argument("--neuroglancer",
                        action='append',
                        help="The Neuroglancer image source, e.g. "
                        "precomputed://http://localhost:9000")
    parser.add_argument("--color",
                        action='append',
                        help="The color scheme to display the Neuroglancer "
                        "image. One of \"gray\", \"red\", \"green\", \"blue\","
                             "\"jet\" or \"cubehelix\".")
    parser.add_argument("--image-name",
                        action="append",
                        help="The name of the image, as displayed in "
                             "Neuroglancer.")
    parser.add_argument("--multiplier",
                        type=float,
                        action="append",
                        help="The multiplier to be applied to the image "
                        "intensity in Neuroglancer.")
    parser.add_argument("--port",
                        type=int,
                        help="HTTP port for neurglancer server",
                        default=0)
    parser.add_argument("--bind-address",
                        help="The IP address to bind to as a webserver. "
                        "The default is 127.0.0.1 which is constrained to "
                        "the local machine.",
                        default="127.0.0.1")
    parser.add_argument("--static-content-source",
                        default=None,
                        help="Obsolete - please don't use")
    parser.add_argument("--n-jobs",
                        type=int,
                        default=-1,
                        help="How many simultaneous jobs to run while training "
                        "or predicting with the random forest.")
    return parser.parse_args()


class MPLCanvas(FigureCanvas):
    def __init__(self, parent):
        figure = Figure()
        self.axes_xy = figure.add_subplot(2, 2, 1)
        self.axes_xz = figure.add_subplot(2, 2, 2)
        self.axes_yz = figure.add_subplot(2, 2, 4)
        super(MPLCanvas, self).__init__(figure)
        self.setParent(parent)
        self.setSizePolicy(QtWidgets.QSizePolicy.Expanding,
                           QtWidgets.QSizePolicy.Expanding)
        self.updateGeometry()

    @staticmethod
    def show1(ax:matplotlib.figure.Axes, images):
        ax.cla()
        if len(images) == 1:
            ax.imshow(images[0], interpolation='bicubic')
        else:
            cimg = np.zeros((images[0].shape[0],
                             images[0].shape[1], 3))
            for i, image in enumerate(images[:3]):
                fimage = (image.astype(float) - np.min(image)) / \
                         max(np.max(image) - np.min(image),
                             np.finfo(float).eps)
                cimg[:, :, i] = fimage
                ax.imshow(cimg, interpolation='bicubic')

    def show(self, image_xy, image_xz, image_yz):
        MPLCanvas.show1(self.axes_xy, image_xy)
        MPLCanvas.show1(self.axes_xz, image_xz)
        MPLCanvas.show1(self.axes_yz, image_yz)
        self.draw()


class ApplicationWindow(QtWidgets.QMainWindow):
    def __init__(self, patches_xy, patches_xz, patches_yz,
                 x, y, z, n_components, use_position,
                 whiten, max_samples, n_jobs,
                 output_file, viewer,
                 neuroglancer_urls,
                 image_names, multipliers, shaders):
        QtWidgets.QMainWindow.__init__(self)
        self.setAttribute(QtCore.Qt.WA_DeleteOnClose)
        self.setWindowTitle("Train")

        self.file_menu = QtWidgets.QMenu('&File', self)
        self.file_menu.addAction("&Save", self.fileSave)
        self.file_menu.addAction("&Write coordinates",
                                 self.fileWriteCoordinates)
        self.file_menu.addAction("&Train", self.fileTrain)
        self.file_menu.addAction('&Quit', self.fileQuit,
                                 QtCore.Qt.CTRL + QtCore.Qt.Key_Q)
        self.menuBar().addMenu(self.file_menu)
        self.save_shortcut = QShortcut(QKeySequence("Ctrl+S"), self)
        self.save_shortcut.activated.connect(self.fileSave)
        self.train_shortcut = QShortcut(QKeySequence("T"), self)
        self.train_shortcut.activated.connect(self.fileTrain)

        self.image_menu = QtWidgets.QMenu("&Image", self)
        self.image_menu.addAction("Ne&xt", self.imageNext)
        self.image_menu.addAction("Next &Positive", self.imageNextPositive)
        self.image_menu.addAction("Next &Negative", self.imageNextNegative)
        self.image_menu.addAction("Next &Unsure", self.imageNextUnsure)
        self.image_menu.addAction("&Go to", self.imageGoTo)
        self.image_menu.addAction("&Brighter", self.imageBrighter,
                                  QtCore.Qt.Key_Plus)
        self.image_menu.addAction("&Dimmer", self.imageDimmer,
                                  QtCore.Qt.Key_Minus)
        self.image_menu.addAction("&Reset brightness",
                                  self.imageResetBrightness)
        self.brightness = 1.0

        self.menuBar().addMenu(self.image_menu)
        self.next_shortcut = QShortcut(QKeySequence("X"), self)
        self.next_shortcut.activated.connect(self.imageNext)
        self.next_positive_shortcut = QShortcut(QKeySequence("Ctrl+P"), self)
        self.next_positive_shortcut.activated.connect(self.imageNextPositive)
        self.next_negative_shortcut = QShortcut(QKeySequence("Ctrl+N"), self)
        self.next_negative_shortcut.activated.connect(self.imageNextNegative)
        self.next_unsure_shortcut = QShortcut(QKeySequence("U"), self)
        self.next_unsure_shortcut.activated.connect(self.imageNextUnsure)
        self.next_go_to_shortcut = QShortcut(QKeySequence("G"), self)
        self.next_go_to_shortcut.activated.connect(self.imageGoTo)

        self.mark_menu = QtWidgets.QMenu("&Mark", self)
        self.mark_menu.addAction("&Positive", self.markPositive)
        self.mark_menu.addAction("&Negative", self.markNegative)
        self.mark_menu.addAction("&Undo", self.markUndo,
                                 QtCore.Qt.CTRL + QtCore.Qt.Key_Z)
        self.menuBar().addMenu(self.mark_menu)

        self.main_widget = QtWidgets.QWidget(self)

        l = QtWidgets.QVBoxLayout(self.main_widget)
        self.title_text = QtWidgets.QLabel()
        l.addWidget(self.title_text)
        self.canvas = MPLCanvas(self.main_widget)
        l.addWidget(self.canvas)

        self.unsure_text = QtWidgets.QLabel()
        l.addWidget(self.unsure_text)
        self.unsure_slider = QtWidgets.QSlider(QtCore.Qt.Horizontal)
        self.unsure_slider.setRange(0, 100)
        self.unsure_slider.setTickInterval(10)
        self.unsure_slider.setValue(50)
        self.unsure_slider.valueChanged.connect(self.on_slider_change)
        l.addWidget(self.unsure_slider)
        self.update_unsure_text()

        self.main_widget.setFocus()
        self.setCentralWidget(self.main_widget)
        icon_path = os.path.join(os.path.dirname(__file__), "training.png")
        self.setWindowIcon(QtGui.QIcon(icon_path))

        self.patches_xy = patches_xy
        self.patches_xz = patches_xz
        self.patches_yz = patches_yz
        n_patches = len(self.patches_xy[0])
        self.patch_len = np.prod(self.patches_xy[0].shape[1:])
        self.n_channels = len(self.patches_xy)
        n_features = self.n_channels * self.patch_len * 3
        self.n_patches = n_patches
        
        # add component by Daye
        self.region_per_idx = []
        self.now_region = 0
        self.region_count = np.zeros(125)
        
        
        self.n_features = n_features
        self.use_position = use_position

        patches = []
        for p in (patches_xy, patches_xz, patches_yz):
            for pc in p:
                patches.append(pc.reshape(n_patches, self.patch_len))
        patches = np.hstack(patches)
        self.x, self.y, self.z = x, y, z
        self.n_jobs = n_jobs
        self.viewer = viewer
        self.neuroglancer_urls = neuroglancer_urls
        self.image_names = image_names
        self.multipliers = multipliers
        self.shaders = shaders
        self.output_file = output_file
        self.predictions = np.zeros(self.n_patches, np.uint8)
        self.pred_probs = np.zeros((self.n_patches, 2), np.float32)

        if os.path.exists(self.output_file):
            with open(self.output_file, "rb") as fd:
                d = pickle.load(fd)
                self.marks = d["marks"]
                self.pca = d["pca"]
                if self.pca.n_components != n_components or\
                        self.pca.whiten != whiten:
                    self.train_pca(max_samples, n_components, whiten, patches)
                else:
                    self.classifier = d["classifier"]
                    if "predictions" in d:
                        self.predictions = d["predictions"]
                    if "pred_probs" in d:
                        self.pred_probs = d["pred_probs"]
                    self.pca_features = np.zeros((len(patches), n_components),
                                                 dtype=np.float32)
                    for idx0 in tqdm.tqdm(range(0, len(patches), max_samples),
                                          desc="PCA transform"):
                        idx1 = min(len(patches), idx0 + max_samples)
                        self.pca_features[idx0:idx1] =\
                            self.pca.transform(patches[idx0:idx1])
                    self.augmented_features = self.augment_features(
                        self.pca_features)

        else:
            self.marks = np.zeros(n_patches, np.int8)
            self.train_pca(max_samples, n_components, whiten, patches)
        self.undo_stack = []
        self.assign_brain_region(self.x, self.y, self.z)
        self.imageNext()

    def train_pca(self, max_samples, n_components, whiten, patches):
        self.classifier = None
        self.pca = PCA(n_components=n_components, whiten=whiten)
        if len(patches) > max_samples:
            idxs = np.random.choice(len(patches), max_samples,
                                    replace=False)
            self.pca.fit(patches[idxs])
            self.pca_features = np.zeros((len(patches), n_components),
                                         dtype=np.float32)
            for idx0 in tqdm.tqdm(range(0, len(patches), max_samples),
                                  desc="PCA transform"):
                idx1 = min(len(patches), idx0 + max_samples)
                self.pca_features[idx0:idx1] = \
                    self.pca.transform(patches[idx0:idx1])
        else:
            self.pca_features = self.pca.fit_transform(patches)
        self.augmented_features = self.augment_features(
            self.pca_features)

    def get_unsure_cutoff_pct(self):
        return self.unsure_slider.value()

    def on_slider_change(self):
        self.update_unsure_text()

    def update_unsure_text(self):
        self.unsure_text.setText(
            "Unsure cutoff percent: %d" % self.get_unsure_cutoff_pct())

    def fileQuit(self):
        self.close()

    def closeEvent(self, ce):
        self.fileQuit()

    def fileSave(self):
        self.statusBar().showMessage("Saving...")
        object = dict(pca=self.pca,
                      classifier=self.classifier,
                      marks=self.marks,
                      predictions=self.predictions,
                      pred_probs=self.pred_probs,
                      x=self.x,
                      y=self.y,
                      z=self.z, brain_region = self.region_per_idx)
        with open(self.output_file, "wb") as fd:
            pickle.dump(object, fd)
        self.statusBar().showMessage("Saved to %s" % self.output_file)

    def fileWriteCoordinates(self):
        """Write a coordinates file using the current positives.

        """
        if self.classifier == None:
            self.statusBar().showMessage("Please train your model")
            return
        mask = self.predictions == 1
        coords = np.column_stack((self.x[mask], self.y[mask], self.z[mask]))
        filename, _ = QtWidgets.QFileDialog.getSaveFileName(
            self, "Save coordinates",
            filter="Coordinates (*.json);;All files (*)"
        )
        if filename:
            with open(filename, "w") as fd:
                json.dump(coords.tolist(), fd)

    def fileTrain(self):
        self.statusBar().showMessage("Training...")
        self.classifier = RandomForestClassifier(
            n_estimators=256,
            class_weight="balanced_subsample",
            oob_score=True,
            n_jobs = self.n_jobs)
        idxs = np.where(self.marks != 0)[0]
        if len(idxs) == 0:
            self.statusBar().showMessage("Nothing to train!")
            return
        n_augment = len(augment(self.patches_xy[0][0],
                                self.patches_xz[0][0],
                                self.patches_yz[0][0]))
        n_patches = n_augment * len(idxs)
        patches = np.zeros((n_patches, self.n_features),
                           self.patches_xy[0].dtype)
        classes = []
        augmented_indexes = []
        for i, idx in enumerate(idxs):
            for channel in range(len(self.patches_xy)):
                for aidx, (pxy, pxz, pyz) \
                        in enumerate(augment(self.patches_xy[channel][idx],
                                             self.patches_xz[channel][idx],
                                             self.patches_yz[channel][idx])):
                    row_idx = i * n_augment + aidx
                    col_idx = channel * self.patch_len
                    for minipatch in (pxy, pxz, pyz):
                        patches[row_idx, col_idx:col_idx+self.patch_len] = \
                            minipatch.flatten()
                        col_idx += self.patch_len * self.n_channels
            for _ in range(n_augment):
                classes.append(0 if self.marks[idx] == -1 else 1)
            augmented_indexes.append(np.ones(n_augment, np.uint32) * idx)
        classes = np.array(classes)
        augmented_indexes = np.hstack(augmented_indexes)
        pca_features = self.augment_features(self.pca.transform(patches),
                                             augmented_indexes)

        self.classifier.fit(pca_features, classes)
        self.statusBar().showMessage("Predicting...")
        self.predictions = self.classifier.predict(self.augmented_features)
        self.pred_probs = self.classifier.predict_proba(self.augmented_features)
        self.statusBar().showMessage("Training complete: oob error = %.3f" %
                                     self.classifier.oob_score_)

    def augment_features(self, pca_features, indexes=None):
        """Augment the feature vector with positional informati;./on

        :param pca_features: the PCA feature vectors
        :param indexes: the indexes of the cells
        """
        if indexes is None:
            indexes = np.arange(self.n_patches)
        if self.use_position:
            return np.column_stack(
                (pca_features,
                 self.x[indexes],
                 self.y[indexes],
                 self.z[indexes]))
        else:
            return pca_features

    def imageNext(self):
        while self.now_region <= 125:
            print(len(np.where(self.region_per_idx == self.now_region)[0]))
            if len(np.where(self.region_per_idx == self.now_region)[0]) != 0:
                break
            self.now_region +=1
         
        
        mask = np.where((self.marks == 0) &
                        (self.region_per_idx == self.now_region))[0]
        self.pick(mask)

    def pick(self, mask):
        if len(mask) == 0:
            self.statusBar().showMessage("No more left")
            return
        self.idx = mask[np.random.randint(0, len(mask))]
        self.show_current()
        
    # add component by Daye 
    def assign_brain_region(self, x, y, z):
        points = np.array([x, y, z]).T

        min_coords = np.min(points, axis=0)
        max_coords = np.max(points, axis=0)

        min_x, min_y, min_z = min_coords
        max_x, max_y, max_z = max_coords

        width = max_x - min_x
        height = max_y - min_y
        depth = max_z - min_z

        cell_size_x = width / 5
        cell_size_y = height / 5
        cell_size_z = depth / 5

        region_per_idx = []
        for point in points:
            if point[0] == max_x: # max 일경우 나누기몫이 5가 되는것 방해
                point[0]=max_x-1 
            if point[1] == max_y:
                point[1]=max_y-1 
            if point[2] == max_z:
                point[2]=max_z-1 
            x_idx = int((point[0] - min_x) // cell_size_x)
            y_idx = int((point[1] - min_y) // cell_size_y)
            z_idx = int((point[2] - min_z) // cell_size_z)
            cell_idx = x_idx + y_idx * 5 + z_idx * 5 * 5
            region_per_idx.append(cell_idx)  # 포인트 당 뇌 영역 index 할당 완료 
        print("Points are assigned to each brain region idx.")
        print(np.unique(np.array(region_per_idx)))
        self.region_per_idx = np.array(region_per_idx)


def show_current(self):
        if self.classifier is not None:
            self.title_text.setText(
                "x=%d, y=%d, z=%d, prob=%.3f" %
                (self.x[self.idx], self.y[self.idx], self.z[self.idx],
                 self.pred_probs[self.idx, 1])
            )
        if self.viewer is not None:
            with self.viewer.txn() as txn:
                txn.position = [
                    self.x[self.idx], self.y[self.idx], self.z[self.idx]]
        self.canvas.show([_[self.idx] for _ in self.patches_xy],
                         [_[self.idx] for _ in self.patches_xz],
                         [_[self.idx] for _ in self.patches_yz])
        self.statusBar().showMessage("Here, brain region is %s, out of %d. This is %d cell in that region." % (self.now_region, len(np.unique(np.array(self.region_per_idx))), self.region_count[now_region]))
    
     def imageNextPositive(self):
        if self.classifier is None:
            self.statusBar().showMessage(
                "Hey, how about training a classifier with the \"T\" key?")
            return
        mask = np.where((self.marks == 0) &
                        (self.predictions == 1) & (self.region_per_idx == self.now_region))[0]
        self.pick(mask)

    def imageNextNegative(self):
        if self.classifier is None:
            self.statusBar().showMessage(
                "Hey, how about training a classifier with the \"T\" key?")
            return
        mask = np.where((self.marks == 0) &
                        (self.predictions == 0) & (self.region_per_idx == self.now_region))[0]
        self.pick(mask)

    def imageNextUnsure(self):
        if self.classifier is None:
            self.statusBar().showMessage(
                "Hey, how about training a classifier with the \"T\" key?")
            return
        unmarked = np.where((self.marks == 0) &
                        (self.region_per_idx == self.now_region))[0]
        unsure_cutoff = self.get_unsure_cutoff_pct() / 100
        order = np.argsort(np.abs(self.pred_probs[unmarked, 1] - unsure_cutoff))
        idx = np.random.randint(0, min(len(order), max(len(order) // 100, 10 )))
        self.idx = unmarked[order[idx]]
        self.show_current()

    def imageGoTo(self):
        with self.viewer.txn() as txn:
            coordinates = txn.voxel_coordinates
            distances = np.sum(np.square(
                np.column_stack((self.x, self.y, self.z)) -
                np.array(coordinates).reshape(1, 3)), 1)
            min_idx = np.argmin(distances)
        self.idx = min_idx
        self.show_current()

    def imageBrighter(self):
        self.brightness = self.brightness * 1.25
        self.update_shader()

    def imageDimmer(self):
        self.brightness = self.brightness / 1.25
        self.update_shader()

    def imageResetBrightness(self):
        self.brightness = 1.0
        self.update_shader()

    def update_shader(self):
        if self.viewer is None:
            return
        with self.viewer.txn() as txn:
            layer(txn, self.image_names[0],
                  self.neuroglancer_urls[0],
                  shader=self.shaders[0],
                  multiplier=self.brightness * self.multipliers[0])

    def markPositive(self):
        print("click")
        self.marks[self.idx] = 1
        # add comonent
        self.region_count[self.now_region] +=1
        if self.region_count[self.now_region] >= 10:
            self.now_region +=1
        self.undo_stack.append(self.idx)
        print("click")

    def markNegative(self):
        self.marks[self.idx] = -1
        # add component
        self.region_count[self.now_region] +=1
        if self.region_count[self.now_region] >= 10:
            self.now_region +=1
        self.undo_stack.append(self.idx)

    def markUndo(self):
        if len(self.undo_stack) == 0:
            self.statusBar().showMessage("Nothing to undo")
            return
        self.idx = self.undo_stack.pop()
        self.marks[self.idx] = 0
        self.show_current()


def augment(xy_patch, xz_patch, yz_patch):
    """Return a sequence reflected and rotated patches

    The Z direction is not symmetric - objects tend to have an anti-shadow
    downward, so we leave that alone and also don't rotate in Z. Arguably,
    there are more subtle artifacts in X and Y that would prevent you from
    switching axes or from flipping directions, but it's probably more
    worthwhile to train on the augmented set than not.

    :param xy_patch:
    :param xz_patch:
    :param yz_patch:
    :return: a sequence with reflected and rotated patches. Each element of
    the sequence is a 3-tuple of patches.
    """
    identity_slice = slice(None)
    reflect_slice = slice(None, None, -1)
    result = []
    for xs, ys, transpose in itertools.product(
            (identity_slice, reflect_slice),
            (identity_slice, reflect_slice),
            (False, True)):
        xy_patch_out = xy_patch[ys, xs]
        xz_patch_out = xz_patch[:, xs]
        yz_patch_out = yz_patch[:, ys]
        if transpose:
            xz_patch_out, yz_patch_out = yz_patch_out, xz_patch_out
            xy_patch_out = xy_patch_out.transpose()
        result.append((xy_patch_out, xz_patch_out, yz_patch_out))
    return result


def main():
    app = QtWidgets.QApplication(sys.argv)
    args = parse_args()
    patches_xy, patches_xz, patches_yz = [[] for _ in range(3)]
    for i, patch_file in enumerate(args.patch_file):
        with h5py.File(patch_file, "r") as fd:
            if len(patches_xy) > 0:
                if len(fd["x"]) != len(x) or \
                   not np.all(fd["x"][:] == x) or\
                   not np.all(fd["y"][:] == y) or\
                   not np.all(fd["z"][:] == z):
                    raise ValueError("The patch files need to be constructed "
                                     "using the same set of blob coordinates")
                if fd["patches_xy"].shape != patches_xy[0].shape:
                    raise ValueError("Patch sizes need to be the same")
            else:
                x = fd["x"][:]
                y = fd["y"][:]
                z = fd["z"][:]
            patches_xy.append(fd["patches_xy"][:])
            patches_xz.append(fd["patches_xz"][:])
            patches_yz.append(fd["patches_yz"][:])
    if args.neuroglancer is None or len(args.neuroglancer) == 0:
        viewer = None
        image_names = None
        multiplier = None
        colors = []
    else:
        import neuroglancer
        import webbrowser
        if args.static_content_source is not None:
            print("--static-content-source is no longer used",
                  file=sys.stderr)
        neuroglancer.set_server_bind_address(
            args.bind_address, bind_port=args.port)
        viewer = neuroglancer.Viewer()
        print("Neuroglancer URL: %s" % str(viewer))
        image_names = args.image_name or []
        while len(image_names) < len(args.neuroglancer):
            image_names.append("image_%d" % (len(image_names) + 1))
        colors = args.color or []
        ckeys = list(COLORS.keys())
        while len(colors) < len(args.neuroglancer):
            for color in ckeys:
                if color not in colors:
                    colors.append(color)
                    break
            else:
                colors.append(ckeys[len(colors) % len(COLORS)])
        multiplier = args.multiplier or []
        while len(multiplier) < len(args.neuroglancer):
            multiplier.append(1.0)

        with viewer.txn() as txn:
            for i in range(len(args.neuroglancer)):
                layer(txn, image_names[i], args.neuroglancer[i],
                      shader=COLORS[colors[i]],
                      multiplier=multiplier[i])

        webbrowser.open_new(viewer.get_viewer_url())
    window = ApplicationWindow(patches_xy, patches_xz, patches_yz,
                               x, y, z, args.n_components,
                               args.use_position,
                               args.whiten,
                               args.max_samples,
                               args.n_jobs, args.output, viewer,
                               args.neuroglancer,
                               image_names,
                               multiplier, [COLORS[_] for _ in colors])
    window.setWindowTitle("Train")
    window.show()
    sys.exit(app.exec())

if __name__=="__main__":
    main()
