import tqdm
import numpy as np
import h5py
import json
import pickle as pkl
import os
import argparse
import sys
import tifffile
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torchvision import transforms
from torchvision import models
import matplotlib.pyplot as plt
import torchvision
from numba import njit
from scipy.spatial.distance import cdist
from scipy.spatial import cKDTree

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--patch-file-tdt",
                       required=True,
                       help="The patch file generated by collect-patches (TdT+ cells)")
    #parser.add_argument("--patch-file-gfp",
                       #required=True,
                       #help="The patch file generated by collect-patches (gfp cells)")
    parser.add_argument("--output-folder",
                        help="The folder name of the output file to be written",
                        required=True)
#     parser.add_argument("--alignment-file",
#                        required=True,
#                        help="The alignmentfile generated by nuggt and neuroglancer")
#     parser.add_argument("--reference-segmentation",
#                         help="The reference segmentation that we map to.",
#                         required=True)
#     parser.add_argument("--brain-regions-csv",
#                         help="The .csv file that provides the correspondences "
#                         "between segmentation IDs and their brain region names",
#                         required=True)
#     parser.add_argument("--level",
#                             help="The granularity level (1 to 8 with 8 as the "
#                                  "finest level.",
#                             type=int,
#                             default=8)
    
    return parser.parse_args()

class pre_to_count():
    def __init__(self, patches_tdt, outputfolder):#,alignmentfile, reference_seg, brain_regions_csv, level):
        self.patches_tdt = patches_tdt
        #self.patches_gfp = patches_gfp
        self.outputfolder = outputfolder
#         self.alignment_file = alignment_file
#         self.refer_seg = reference_seg
#         self.region_csv = brain_regions_csv
#         self.level = level
        
        files = [self.patches_tdt]
        X_stack = np.empty((0, 3, 31, 31))
        self.coords_stack = np.empty((0, 3))
        
        for file in files: # with several files
            with h5py.File(file, "r") as f:
                patches_xy = f[list(f.keys())[1]][()]
                patches_xz = f[list(f.keys())[2]][()]
                patches_yz = f[list(f.keys())[3]][()]
                X = np.stack((patches_xy, patches_xz, patches_yz), axis=1)
                X_stack = np.vstack((X_stack, X))
                x = f[list(f.keys())[4]][()]
                y = f[list(f.keys())[5]][()]
                z = f[list(f.keys())[6]][()]
                coords = np.stack((x, y, z), axis=1)
                self.coords_stack = np.vstack((self.coords_stack, coords))
        X_stack_t = torch.from_numpy(X_stack.astype(np.float64))
        transform = transforms.Compose([
            transforms.Resize((224, 224), antialias=True),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        X_stack_tt = transform(X_stack_t)  # 입력 이미지 데이터
        X_stack_ttf = X_stack_tt.float() #.to("cuda")
        print("Total points..: ", np.array(X_stack_ttf).shape)
        self.X_stack_ttf = X_stack_ttf
        
        
    def predict_model(self):
        batch_size = 32
        predict_dataset = TensorDataset(self.X_stack_ttf)
        predict_loader = DataLoader(predict_dataset, batch_size=batch_size, shuffle=False)
        
        pretrained_model = models.resnet18(weights=None) #False)

        # 기존의 마지막 레이어 (클래스 분류용)를 제거
        num_features = pretrained_model.fc.in_features
        pretrained_model.fc = nn.Identity()  # 기존의 마지막 레이어를 제거

        class BinaryClassifier(nn.Module):
            def __init__(self):
                super(BinaryClassifier, self).__init__()
                self.fc = nn.Sequential(
                    nn.Linear(num_features, 256),
                    nn.ReLU(),
                    nn.Dropout(0.5),
                    nn.Linear(256, 1),  # 1개의 출력 뉴런 (이진 분류)
                    nn.Sigmoid()  # 이진 분류를 위한 시그모이드 활성화 함수
                )

            def forward(self, x):
                x = self.fc(x)
                return x

        # 이진 분류 모델 생성
        binary_model = BinaryClassifier()

        for param in pretrained_model.parameters():
            param.requires_grad = True

        pretrained_model = nn.Sequential(pretrained_model, binary_model)
        pretrained_model = pretrained_model.to('cuda')

        criterion = nn.BCELoss()  # 이진 분류 손실 함수
        optimizer = optim.Adam(pretrained_model.parameters(), lr=0.0001)
        
        # later model path add? 
        pretrained_model.load_state_dict(torch.load('/data3/DayeKim/main/9brains_half_resnet_model_0.9318.pth'))
        pretrained_model.eval()  # 모델을 평가 모드로 전환
        total_correct = 0
        total_samples = 0

        predicted_stack = np.array([])
        label_stack = np.array([])

        with torch.no_grad():
            for images in predict_loader:
                images, = images
                images = images.to("cuda")
                outputs = pretrained_model(images)
                predicted = (outputs >= 0.5).int()
                #print("predicted stack", predicted_stack, predicted_stack.shape)
                #print("pre", predicted.shape[0], "sq", predicted.squeeze().shape[0])
                if predicted.shape[0] == 32:
                    predicted_stack = np.concatenate((predicted_stack, predicted.squeeze().cpu()))
                else: 
                    print(predicted_stack.shape, predicted.shape, "pred", predicted, "pred0", predicted[0])
                    predicted_stack = np.concatenate((predicted_stack, predicted[0].cpu()))
        
        positive_sr = predicted_stack[np.where(predicted_stack==1)]
        positive_coords = self.coords_stack[np.where(predicted_stack==1)]
        self.positive_coords = positive_coords
        
        file_path = self.outputfolder + '/positive_predicted_points.json'
        #save 
        with open(file_path, 'w') as f:
            json.dump(self.positive_coords.tolist(), f)
        print("predicted_positive_cells_shape", self.positive_coords.shape)
        return None

    def source_area_check(self):
        print(np.array(self.positive_coords).shape)
        indices = source_check.check_sourcearea(self.positive_coords, 200, 100, 10) 
        ##distance within, nearest distance
        self.tdt_source_area = self.positive_coords[indices]
        self.other_points = self.positive_coords[~indices]
        self.whole_area_cells_positive = np.vstack([self.other_points, self.tdt_source_area])

        #save 
        file_path_r = self.outputfolder + '/positive_recipientarea_cells_tdt.json'
        with open(file_path_r, 'w') as f:
            json.dump(self.other_points.tolist(), f)
        
        file_path_s = self.outputfolder + '/positive_sourcearea_cells_tdt.json'
        with open(file_path_s, 'w') as f:
            json.dump(self.tdt_source_area.tolist(), f)
            
        file_path_w = self.outputfolder + '/positive_wholearea_cells_tdt.json'
        with open(file_path_w, 'w') as f:
            json.dump(self.whole_area_cells_positive.tolist(), f)
        
        print("tdt_source_area_p_cells", self.tdt_source_area.shape)
        print("tdt_recipient_area_p_cells", self.other_points.shape)
        print("tdt_whole_area_cells_positive", self.whole_area_cells_positive.shape)       
    

    def celltype_check_withgfp(self):
        self.source_cell_tdt = []
        with open(self.patches_gfp, 'r') as f:
            gfp_points = np.array(json.load(f))
    
        for g_point in tqdm.tqdm(gfp_points):
            
            distances = cdist([g_point], self.tdt_source_area)
            ind = np.argmin(distances)
            dist = distances[0, ind]

            if dist <= 100:
                self.source_cell_tdt.append(self.tdt_source_area[ind].copy().tolist())
                self.tdt_source_area[ind] = np.array([np.inf, np.inf, np.inf])
        
        A = np.array(self.whole_area_cells_positive).astype(int).copy()
        B = np.array(self.source_cell_tdt).astype(int).copy()

        # A에서 B와 중복되는 행 제거
        result = []
        for x in tqdm.tqdm(A):
            if not np.any(np.all(x == B, axis=1)):
                result.append(x)

        self.recipient_cells = np.array(result)
        print()        
        print("source cell : ", np.array(self.source_cell_tdt).shape) # source cell 
        print("recipient cell : ", self.recipient_cells.shape)
        print("total positive cell ", self.whole_area_cells_positive.shape) # whole positive
        
        file_path_source = self.outputfolder + '/source_cells_total.json'
        file_path_recipient = self.outputfolder + '/recipient_cells_total.json'

        # 리스트를 JSON 파일로 저장
        with open(file_path_source, 'w') as f:
            json.dump(self.source_cell_tdt, f)

        with open(file_path_recipient, 'w') as f:
            json.dump(self.recipient_cells.tolist(), f)

        return None

    def plot_cells(self, source, recipient):
        densest_cluster_points = np.array(self.source_cell_tdt).copy()
        other_points = self.recipient_cells.copy()

        plt.clf()
        fig = plt.figure(figsize=(10, 10))  # 전체적인 그래픽 크기 조절

# 3차원 subplot 4개 생성
        ax1 = fig.add_subplot(221, projection='3d')  # 첫 번째 subplot
        ax2 = fig.add_subplot(222, projection='3d')  # 두 번째 subplot
        ax3 = fig.add_subplot(223, projection='3d')  # 세 번째 subplot
        ax4 = fig.add_subplot(224, projection='3d')  # 네 번째 subplot

        for ax in [ax1, ax2, ax3, ax4]:
            if source==1:
                ax.scatter(densest_cluster_points[:, 0], densest_cluster_points[:, 1], densest_cluster_points[:, 2], s=5, c='blue', label='source_cell_referring_gfp')
                imagen = "source_plot.png"
            if recipient==1:
                ax.scatter(other_points[:, 0], other_points[:, 1], other_points[:, 2], s=5, c='red', label='recipient_cell')
                imagen = "recipient_plot.png"
            if source == 1 and recipient == 1:
                imagen = "source_recipient_plot.png"
            
            ax.set_xlabel('X')
            ax.set_ylabel('Y')
            ax.set_zlabel('Z')
            ax.legend()

# subplot별로 각도 조정
        ax1.view_init(30, 120)  # (elev, azim)
        ax2.view_init(45, 30)
        ax3.view_init(60, 240)
        ax4.view_init(75, 60)

# 모든 subplot에서 x, y, z 범위 일치시키기
        for ax in [ax1, ax2, ax3, ax4]:
            ax.set_xlim(np.min(self.whole_area_cells_positive[:, 0]), np.max(self.whole_area_cells_positive[:, 0]))
            ax.set_ylim(np.min(self.whole_area_cells_positive[:, 1]), np.max(self.whole_area_cells_positive[:, 1]))
            ax.set_zlim(np.min(self.whole_area_cells_positive[:, 2]), np.max(self.whole_area_cells_positive[:, 2]))
        plt.savefig(self.outputfolder + "/" + imagen)
        plt.show()
        return None 

class source_check():
    @staticmethod
    @njit
    def calculate_density(points, radius):
        density = np.zeros(len(points), dtype=np.int64)
        n = len(points)
        for i in range(n):
            diffs = points - points[i]
            dist_squared = np.sum(diffs ** 2, axis=1)
            sq_dist_squared = np.sqrt(dist_squared)
            density[i] = np.sum(sq_dist_squared <= radius) - 1
        return density

    @staticmethod
    def calculate_distance(points):
        kdtree = cKDTree(points)
        distances, _ = kdtree.query(points, k=2)  # 가장 가까운 이웃 2개 찾기 (자기 자신과의 거리 포함)
        nearest_distances = distances[:, 1]
        return nearest_distances

    @staticmethod
    def check_sourcearea(points, radius, distance, counts):
        density = source_check.calculate_density(points, radius)
        nearest_distances = source_check.calculate_distance(points) 
        mask = (density >= counts) & (nearest_distances <= distance)
        return mask

def main():
    print("run...")
    args = parse_args()
    img_pros = pre_to_count(args.patch_file_tdt, args.output_folder)#, args.alignment_file, args.reference_segmentation, args.brain_regions_csv, args.level)
    print()
    print("Predict tdt+ cells...")
    img_pros.predict_model() 
    print()
    #print("Check the source area based on tdt+ cells...")
    #img_pros.source_area_check()
    print()
    #print("Finding the source/recipient cell based on GFP cells(assign)...")
    #img_pros.celltype_check_withgfp()
    print()
    #print("Counting the source/recipient cell in each brain regions...")
#         cell_counting()
    #print("Done! Let's check the results.")
    #print("tips: Close the tap if you completed checking the plot. then it will continue to run")
    #print("1. Plot the source and recipient cells...")
    #img_pros.plot_cells(1,1) 
    #print("2. Plot only the source cells...")
    #img_pros.plot_cells(1,0)
    #print("3. Plot only the recipient cells...")
    #img_pros.plot_cells(0,1) 
    #print()
    #print("final files : source_cells_total.json, recipient_cells_total.json")
    print("Process is finished. Please check the output folder you entered.")
    print("I'm out!")
    return None 

if __name__=="__main__":
    main()
